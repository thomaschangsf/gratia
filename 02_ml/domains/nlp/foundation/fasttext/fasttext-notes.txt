// -------------------------------------
// References
// -------------------------------------
<-- https://amitness.com/2020/06/fasttext-embeddings/ 
	* Wordvec and fasttext creates word embeddings.

	* Wordvec improvements
		- Wordvec treats eats and eaten as 2 different words. 
		- Out of Vocab word

	* Fasttext
		- Addresses/mitigates both word2vec limitations converting the target word into ngrams. It then averages the ngrams to represent the target word.
		- Only ngrams is generated on the target word, the context word is original word.
			Ex: am eating food
				target=eating  -> avg(ea, at, ti, in, ng) -
				context=am, food
		- To minimize size due to the ngram, we hash the ngram into buckets
		- For the loss function, 
			we maximize the distance between the target and context words toward 1 using SGD optimizer. 
				update weights so sigmoid( dot(embedding_target, embedding_context) )  --> toward 1
			
			we minimize the distance between the target and random selected wrds (negative sampling)





<-- Documentation: https://fasttext.cc/docs/en/faqs.html
	* Reduce model size by:
		- reduce the hashtable for ngram.  
		- reduce dimension size
		- add quantization mode
			See paper on How Quantization works

		- train_supervised api: play around with dim, bucket, 
		    input             # training file path (required)
		    lr                # learning rate [0.1]
		    dim               # size of word vectors [100]
		    ws                # size of the context window [5]
		    epoch             # number of epochs [5]
		    minCount          # minimal number of word occurences [1]
		    minCountLabel     # minimal number of label occurences [1]
		    minn              # min length of char ngram [0]
		    maxn              # max length of char ngram [0]
		    neg               # number of negatives sampled [5]
		    wordNgrams        # max length of word ngram [1]
		    loss              # loss function {ns, hs, softmax, ova} [softmax]
		    bucket            # number of buckets [2000000]
		    thread            # number of threads [number of cpus]
		    lrUpdateRate      # change the rate of updates for the learning rate [100]
		    t                 # sampling threshold [0.0001]
		    label             # label prefix ['__label__']
		    verbose           # verbose [2]
		    pretrainedVectors # pretrained word vectors (.vec file) for supervised learning []


	* API: 
		python: https://fasttext.cc/docs/en/python-module.html
			has both supervised and unsupervised api

		C: https://fasttext.cc/docs/en/html/index.html



<-- Paper: Bags of Tricks for Efficient Text Classification
	* Location: /Users/chang/Documents/dev/git/ml-tools/ml/nlp/fasttext/paper
	* Capability
		Train on billions of words in less than 10 minutes on multicore CPU
		Support up to 312K classes

	* For optimization, hieararchical softmax reduces computation complexity of inference
		- Given k classes and embedding of h dimenstion, the inference complexity
			Linear classifer: O(h k)
			Fasttext: O(h log_2(k) )


<-- Paper: How Quantization works
	* Intuition: Reduce number of embeddings by encouraging words to share embeddings.  WordA is compose of N embeddings. Each of the N embeddings can be assocated with other words.



