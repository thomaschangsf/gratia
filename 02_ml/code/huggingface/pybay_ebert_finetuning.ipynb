{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- Notebook shows how to use huggingface to fine tune using Ebert embeddings\n",
    "- Based on https://pages.github.corp.ebay.com/PyBay/core/dev/explore/bert/tutorials/finetune.html#finetuning\n",
    "- Other reference: https://pages.github.corp.ebay.com/PyBay/core/dev/explore/bert/introduction.html#getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8a524d7fd41cc35d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/thchang/.cache/huggingface/datasets/csv/default-8a524d7fd41cc35d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/thchang/.cache/huggingface/datasets/csv/default-8a524d7fd41cc35d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "DATA_DIR = '/data/ebay/data/ppetrushkov/relevance'\n",
    "data = datasets.load_dataset(\n",
    "    'csv',\n",
    "    data_files={\n",
    "        'train': os.path.join(DATA_DIR, 'search_relevance.us.train.csv'),\n",
    "        'dev': os.path.join(DATA_DIR, 'search_relevance.us.dev.csv'),\n",
    "    },\n",
    ")\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'title', 'relevance'],\n",
       "        num_rows: 864378\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query', 'relevance', 'title'],\n",
       "        num_rows: 45970\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'title', 'relevance'],\n",
       "    num_rows: 864378\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8030e32e6c4da09394213dc423c424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='downloading', max=5.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21eef7fe8f44496e91bb7c1a1dd445cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='downloading', max=5.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea89b351811d43139d13edac303d5175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=865.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d506ab118a0f4f5d89ceb0b70c3612bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pybay.bert\n",
    "\n",
    "# Load pretrained eBERT tokenizer. Learn more about tokenizers https://huggingface.co/docs/tokenizers/python/latest/\n",
    "tokenizer = pybay.bert.AutoTokenizer.from_pretrained('eBERT-multilingual-base-2020Q3-cased')\n",
    "\n",
    "# Preprocessing function that will be applied to all dataset entries\n",
    "def preprocess(features):\n",
    "    # We combine a query and a title into one sentence, separated by a special token\n",
    "    result = tokenizer([' [SEP] '.join([query, title]) for query, title in zip(features['query'], features['title'])])\n",
    "    # We convert the relevance score into an integer label\n",
    "    result['label'] = [int(x) for x in features['relevance']]\n",
    "    return result\n",
    "\n",
    "# Apply preprocessing to all entries in the dataset\n",
    "data = data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, average_precision_score,\n",
    "                            roc_auc_score, mean_squared_error)\n",
    "\n",
    "# This function will be called to evaluate a prediction, which contains model output and a label\n",
    "def compute_metrics(eval_predictions: EvalPrediction):\n",
    "    # model returns logits of shape [n_samples, n_classes]\n",
    "    logits = eval_predictions.predictions\n",
    "    # logit with the largest value is our class prediction\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    # true label\n",
    "    trues = eval_predictions.label_ids\n",
    "\n",
    "    # We use metrics provided by scikit-learn\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_true=trues, y_pred=predictions),\n",
    "        \"BalancedAccuracy\": balanced_accuracy_score(y_true=trues, y_pred=predictions),\n",
    "        \"AveragePrecision\": average_precision_score(y_true=trues, y_score=logits[:, 1], average='weighted', pos_label=1),\n",
    "        \"ROCAUCScore\": roc_auc_score(y_true=trues, y_score=logits[:, 1], average='weighted'),\n",
    "        \"MSE\": mean_squared_error(y_true=trues, y_pred=predictions, squared=False),\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.6666666666666666, 'BalancedAccuracy': 0.75, 'AveragePrecision': 1.0, 'ROCAUCScore': 1.0, 'MSE': 0.5773502691896257}\n"
     ]
    }
   ],
   "source": [
    "### Sample metric with mock data\n",
    "prediction = EvalPrediction(\n",
    "    predictions=np.array([[0.0, 1.0], [1.0, 0.0], [-3.0, 5.0]]),\n",
    "    label_ids=np.array([0, 0, 1])\n",
    ")\n",
    "print(compute_metrics(prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Prepare HuggingFace Model and Trainer\n",
    "- Next we will load our eBERT model and setup a transformers.Trainer instance to fine-tune it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849efab7479942d985a575c149bd634b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='downloading', max=5.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfdb47d92584eadab3995d918defd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='downloading', max=5.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/ebay/data/thchang/.pybay_cache/e6f108c7-44e2-49cc-9809-78dfdfa0eb1f were not used when initializing EBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing EBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EBertForSequenceClassification were not initialized from the model checkpoint at /data/ebay/data/thchang/.pybay_cache/e6f108c7-44e2-49cc-9809-78dfdfa0eb1f and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This loads a pytorch eBERT model, which is initialized with pre-trained weights.\n",
    "# This model also contains some additional weights, which are used for classification on top of pre-trained BERT output.\n",
    "# Those weights are randomly initialized, so the model needs to be fine-tuned to produce any meaningful output.\n",
    "# Read more https://huggingface.co/transformers/model_doc/auto.html#automodelforsequenceclassification\n",
    "model = pybay.bert.AutoModelForSequenceClassification.from_pretrained('eBERT-multilingual-base-2020Q3-cased',\n",
    "                                                                      id2label={\n",
    "                                                                          0: 'irrelevant',\n",
    "                                                                          1: 'relevant',\n",
    "                                                                      })\n",
    "\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "# Define an output directory\n",
    "output_dir = os.path.join(os.environ.get('KRYLOV_DATA_DIR', ''), os.environ.get('KRYLOV_WS_PRINCIPAL', ''), 'bert-finetuning')\n",
    "\n",
    "# Learn more about various training arguments https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, 'model_checkpoints'),       # Output model checkpoints\n",
    "    num_train_epochs=2,                                             # Number of training epochs\n",
    "    per_device_train_batch_size=64,                                 # Batch size for training\n",
    "    per_device_eval_batch_size=64,                                  # Batch size for evaluation\n",
    "    learning_rate=5e-6,                                             # Learning rate\n",
    "    warmup_steps=100,                                               # Warmup for learning rate schedule\n",
    "    logging_steps=5000,                                             # Logging frequency\n",
    "    logging_dir=os.path.join(output_dir, 'model_logs'),             # Directory for logs\n",
    "    fp16=True,                                                      # Mixed precision training on V100 GPUs\n",
    "    save_steps=0,                                                   # Checkpoint saving frequency\n",
    ")\n",
    "\n",
    "# Learn more about Trainer https://huggingface.co/transformers/main_classes/trainer.html#id1\n",
    "# data_collator will take multiple input entries and create a mini-batch. Because\n",
    "# input sentences can have different length, it will add appropriate padding and\n",
    "# masking, so that the model can efficiently and correctly process it.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['dev'],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27012' max='27012' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27012/27012 39:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.324200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='719' max='719' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [719/719 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34215739369392395, 'eval_Accuracy': 0.8670002175331738, 'eval_BalancedAccuracy': 0.7625003942253656, 'eval_AveragePrecision': 0.9540019136251331, 'eval_ROCAUCScore': 0.8750029086247337, 'eval_MSE': 0.36469135233348515, 'eval_runtime': 37.2114, 'eval_samples_per_second': 1235.373, 'eval_steps_per_second': 19.322, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/data/ebay/data/thchang/bert-finetuning/my_pretrained_model/tokenizer_config.json',\n",
       " '/data/ebay/data/thchang/bert-finetuning/my_pretrained_model/special_tokens_map.json',\n",
       " '/data/ebay/data/thchang/bert-finetuning/my_pretrained_model/vocab.txt',\n",
       " '/data/ebay/data/thchang/bert-finetuning/my_pretrained_model/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# This will start a training loop\n",
    "trainer.train()\n",
    "\n",
    "# Final evaluation on a dev set\n",
    "results = trainer.evaluate(data['dev'])\n",
    "print(results)\n",
    "\n",
    "# Save the final model and tokenizer to disk\n",
    "model.save_pretrained(os.path.join(output_dir, 'my_pretrained_model'))\n",
    "tokenizer.save_pretrained(os.path.join(output_dir, 'my_pretrained_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Wrap in a Service/Inference Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"relevant\"]\n",
      "[\"irrelevant\"]\n",
      "[\"relevant\"]\n"
     ]
    }
   ],
   "source": [
    "# This will load and setup our saved tokenizer and model.\n",
    "# We only need to provide a mapping from class indices, to some meaningful class names\n",
    "classifier = pybay.bert.EBertTextClassifier(os.path.join(output_dir, 'my_pretrained_model'))\n",
    "\n",
    "# Let's see how our model performs on some samples\n",
    "print(classifier.classify(\"headphones [SEP] JBL - LIVE 500BT\"))\n",
    "print(classifier.classify(\"headphones [SEP] JBL - LIVE 500BT headphones stand\"))\n",
    "print(classifier.classify(\"headphones [SEP] quietcomfort 35\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start uvicorn service \n",
    "- Run this in a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "export MODEL_PATH=/data/ebay/data/ppetrushkov/bert-finetuning\n",
    "export MODEL_CLASS=EBertTextClassifier\n",
    "uvicorn --host 0.0.0.0 --port 5000 pybay.bert.app:app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
