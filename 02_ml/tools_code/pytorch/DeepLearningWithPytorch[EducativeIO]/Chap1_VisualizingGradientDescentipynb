{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTxGXDUQzl5b"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijJrpbXSzl5e"
   },
   "source": [
    "# Chapter 2: Visualizing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO-ddD4Nzl5e"
   },
   "source": [
    "Now that you've learned how gradient descent works, it's time to put your knowledge into action :-)\n",
    "\n",
    "We're generating a new synthetic dataset using *b = 0.5* and *w = -3* for a **linear regression with a single feature (x)**:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "y = b + w x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SROq0QVzl5f"
   },
   "source": [
    "You'll implement the **five steps** of gradient descent in order to **learn these parameters** from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqVyUJ0ozl5f"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwewZkWRzl5g"
   },
   "outputs": [],
   "source": [
    "true_b = .5\n",
    "true_w = -3\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt-F3zuazl5g"
   },
   "source": [
    "## Step 0: Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl0mkj_qzl5g"
   },
   "source": [
    "The first step - actually, the zeroth step - is the *random initialization* of the parameters. Using Numpy's `random.randn` method, you should write code to initialize both *b* and *w*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omCi6gpWzl5g"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gs8PxIRHzl5h"
   },
   "outputs": [],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7grxz7Cozl5i"
   },
   "source": [
    "## Step 1: Compute Model's Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HBAlaNUzl5i"
   },
   "source": [
    "The first step (for real) is the **forward pass**, that is, the **predictions** of the model. Our model is a linear regression with a single feature (x), and its parameters are *b* and *w*. You should write code to generate predictions (yhat):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z0il_cYzl5i"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V1FDXZczl5i"
   },
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBPJyqpuzl5j"
   },
   "source": [
    "## Step 2: Compute the Mean Squared Error (MSE) Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBO_BBEuzl5j"
   },
   "source": [
    "Since our model is a linear regression, the appropriate loss is the **Mean Squared Error (MSE)** loss:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "error_i = \\hat{y_i} - y_i\n",
    "\\\\\n",
    "\\Large\n",
    "loss = \\frac{1}{N}\\sum_{i=0}^N{error_i^2}\n",
    "$$\n",
    "\n",
    "For each data point (i) in our training set, you should write code to compute the difference between the model's predictions (yhat) and the actual values (y_train), and use the errors of all N data points to compute the loss:\n",
    "\n",
    "Obs.: DO NOT use loops!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rudCYkv3zl5j"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9STCHdYYzl5j"
   },
   "outputs": [],
   "source": [
    "# Step 2 - Computing the loss\n",
    "# We are using ALL data points, so this is BATCH gradient\n",
    "# descent. How wrong is our model? That's the error!\n",
    "error = (yhat - y_train)\n",
    "\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWE0yD9Azl5k"
   },
   "source": [
    "## Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBKF7uEOzl5k"
   },
   "source": [
    "PyTorch's autograd will take care of that later on, so we don't have to compute any derivatives yourself! So, no need to manually implement this step.\n",
    "\n",
    "You *still* should understand what the gradients *mean*, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lf-KNLyzl5k"
   },
   "outputs": [],
   "source": [
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "b_grad = 2 * error.mean()\n",
    "w_grad = 2 * (x_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z56WLp--zl5l"
   },
   "source": [
    "The gradients above indicate that:\n",
    "- for a tiny increase in the value of the parameter *b*, the loss will increase roughly 2.7 times as much\n",
    "- for a tiny increase in the value of the parameter *w*, the loss will increase roughly 1.8 times as much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCy-JIE7zl5l"
   },
   "source": [
    "## Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX3n1t8Jzl5l"
   },
   "source": [
    "The fourth step is the **parameter update** - you should write code that use the gradients and a learning rate (set to 0.1) to update the parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6KvBpH0zl5l"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozq6Lspozl5l"
   },
   "outputs": [],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 4 - Updates parameters using gradients and the \n",
    "# learning rate\n",
    "b = b - lr * b_grad\n",
    "w = w - lr * w_grad\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxYilJgwzl5m"
   },
   "source": [
    "## Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoNLjWtPzl5m"
   },
   "source": [
    "The last step consists of putting the other steps together and organize them inside a loop. Write code to fill in the blanks in the loop below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adlXOhh0zl5m"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwnxi_cVzl5m"
   },
   "outputs": [],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1: Forward pass\n",
    "    yhat = b + w * x_train\n",
    "    \n",
    "    # Step 2: Compute MSE loss\n",
    "    error = (yhat - y_train)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Step 3: Compute the gradients\n",
    "    b_grad = 2 * error.mean()\n",
    "    w_grad = 2 * (x_train * error).mean()\n",
    "\n",
    "    # Step 4: Update the parameters\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "    \n",
    "print(b, w)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AxCvsFtzl5n"
   },
   "source": [
    "Congratulations! Your model is able to learn both *b* and *w* that are **really close** to their true values. They will never be a perfect match, though, because of the *noise* we added to the synthetic data (and that's always present in real world data!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rk_7CP5Gzl5n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenges01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
