{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1878990",
   "metadata": {},
   "source": [
    "# 0 Goal\n",
    "- Using Hugging Face Dataset\n",
    "- Follow EducativeIO StepByStep class/flow to train a multi-class classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75147be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "619a01bf",
   "metadata": {},
   "source": [
    "# 1 Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c74da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "#import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "dir_base=\"/Users/chang/Documents/dev/git/opus/04_ml/general/pytorch/PyTorchNLPBook\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ced468",
   "metadata": {},
   "source": [
    "# 2 HuggingFace \n",
    "- 1. Load dataset from HuggingFace\n",
    "- 2. Play with some HF Dataset operators\n",
    "- 3. Convert to Torch Dataset and Dataloadersm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bc92e",
   "metadata": {},
   "source": [
    "### 2.1 Load Dataset from HuggingFace\n",
    "- https://huggingface.co/datasets/viewer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793d7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "datasets_list = list_datasets()\n",
    "#datasets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c4118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/chang/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbb7d0f14ab48998b2ca598563231aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/viewer/\n",
    "from datasets import load_dataset\n",
    "dataset_agnews_train, dataset_agnews_test = load_dataset('ag_news', split=['train', 'test'])\n",
    "#dataset_amazon_review = load_dataset('amazon_us_reviews', 'Wireless_v1_00', split='train[:10%]') # take only 10% of data; amazonreview is 1.7GB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1218ef",
   "metadata": {},
   "source": [
    "### 2.2 How to manipuate HF Dataset\n",
    "- https://huggingface.co/docs/datasets/processing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291fd01a",
   "metadata": {},
   "source": [
    "#### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c6bc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 120000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(type(dataset_agnews_train))\n",
    "dataset_agnews_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b9b99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=4, names=['World', 'Sports', 'Business', 'Sci/Tech'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_agnews_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dcfb30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\"],\n",
       " 'label': [2]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset_agnews_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e05e0ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_agnews_train.features['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e659ecd",
   "metadata": {},
   "source": [
    "#### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6367436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace split\n",
    "dataset_agnews_train.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bc5cf",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac2d3ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/chang/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-ca8d37282eca128f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['Wall Street to Open Little Changed  NEW YORK (Reuters) - Wall Street is seen opening little  changed on Monday as crude prices remain high, but insurers may  dip on worries about their potential liabilities after a  hurricane struck Florida on Friday.'],\n",
       " 'label': [2]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_agnews_train.filter(lambda e: e['text'].startswith('Wall Street'))[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a5528",
   "metadata": {},
   "source": [
    "### 2.3 Convert HF Dataset with Pytorch Dataset and DataLoader\n",
    "- https://huggingface.co/docs/datasets/torch_tensorflow.html\n",
    "\n",
    "- One can convert HF to different formats (ie Torch, TF, pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9610a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/chang/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-1ea5f6dcf9161ad3.arrow\n"
     ]
    }
   ],
   "source": [
    "# First use HF dataset and pre-process with Bert\n",
    "from transformers import AutoTokenizer\n",
    "#dataset = load_dataset('glue', 'mrpc', split='train')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# To use datasets.Dataset.map() to update elements one needs provide a function with the following signature: \n",
    "# function(example: dict) -> dict\n",
    "def preprocess(e):\n",
    "    return tokenizer(e['text'], truncation=True, padding='max_length')\n",
    "\n",
    "dataset_dev = dataset_agnews_train.map(preprocess, batched=True, num_proc=2)\n",
    "dataset_test = dataset_agnews_test.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c84af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f98299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to TORCH and keep which columns\n",
    "dataset_dev.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_test.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define torch Dataloader\n",
    "dataloader_dev = torch.utils.data.DataLoader(dataset_dev, batch_size=32)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc990f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "    num_rows: 120000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc65b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3ebed95",
   "metadata": {},
   "source": [
    "# 2 Educative IO Torch Pipeline\n",
    "- References:\n",
    "    * Notes on Google Cloud Pytorch\n",
    "    * Notebook: /Users/chang/Documents/dev/git/opus/04_ml/general/pytorch/DeepLearningWithPytorch[EducativeIO]/Chap5_ClassificationProb.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd4351",
   "metadata": {},
   "source": [
    "### 2.1 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68f76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chang/Documents/dev/git/opus/04_ml/general/pytorch/.venv/lib/python3.9/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step = self._make_train_step()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step = self._make_val_step()\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(\n",
    "            folder, name, suffix\n",
    "        ))\n",
    "\n",
    "    def _make_train_step(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step\n",
    "    \n",
    "    def _make_val_step(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, \n",
    "            # since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step = self.train_step\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the \n",
    "        # same mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28042846",
   "metadata": {},
   "source": [
    "### 2.2 Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04487132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential()\n",
    "model.add_module('linear', nn.Linear(2, 1))\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a BCE loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f023ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c8a6740",
   "metadata": {},
   "source": [
    "### 2.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947a9de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f2714d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lm/cz7x2xr14rjcbgdmgkg156vr0000gn/T/ipykernel_9832/3362851327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lm/cz7x2xr14rjcbgdmgkg156vr0000gn/T/ipykernel_9832/3853321514.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, seed)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# inner loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# Performs training using mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lm/cz7x2xr14rjcbgdmgkg156vr0000gn/T/ipykernel_9832/3853321514.py\u001b[0m in \u001b[0;36m_mini_batch\u001b[0;34m(self, validation)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# same mini-batch loop we had before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mmini_batch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "sbs = StepByStep(model, loss_fn, optimizer) \n",
    "\n",
    "# Issue is that we have a bert sequence format, but SBS is expecting a tensor \n",
    "# TODO: convert model to one that takes that input; look at Michael's code !!!\n",
    "# Example: \n",
    "#.   Wrap HF Bert inside Torch model\n",
    "#.   https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/\n",
    "sbs.set_loaders(dataloader_dev, dataloader_test) \n",
    "\n",
    "sbs.train(n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2afe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192e78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22f905a5",
   "metadata": {},
   "source": [
    "### 2.3 Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the parameter values of the Linear model\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4df43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f25e0fe",
   "metadata": {},
   "source": [
    "### 2.4 Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ac4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_val = sbs.predict(X_val)\n",
    "probabilities_val = sigmoid(logits_val).squeeze()\n",
    "cm_thresh50 = confusion_matrix(y_val, (probabilities_val >= 0.5))\n",
    "print(cm_thresh50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e2f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15a808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115cc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff77f5a6",
   "metadata": {},
   "source": [
    "# 6 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf196776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    vectorized_surname, vec_length = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
    "    vec_length = torch.tensor([vec_length], dtype=torch.int64)\n",
    "    \n",
    "    result = classifier(vectorized_surname, vec_length, apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    \n",
    "    index = indices.item()\n",
    "    prob_value = probability_values.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': prob_value, 'surname': surname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1a8b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nationality': 'Irish', 'probability': 0.2552226781845093, 'surname': 'McMahan'}\n",
      "{'nationality': 'Japanese', 'probability': 0.793895959854126, 'surname': 'Nakamoto'}\n",
      "{'nationality': 'Chinese', 'probability': 0.39525306224823, 'surname': 'Wan'}\n",
      "{'nationality': 'Korean', 'probability': 0.3733985126018524, 'surname': 'Cho'}\n",
      "{'nationality': 'Chinese', 'probability': 0.4022286534309387, 'surname': 'Chang'}\n",
      "{'nationality': 'Vietnamese', 'probability': 0.44121870398521423, 'surname': 'Vuc'}\n"
     ]
    }
   ],
   "source": [
    "# surname = input(\"Enter a surname: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho', 'Chang', 'Vuc']:\n",
    "    print(predict_nationality(surname, classifier, vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7702866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
