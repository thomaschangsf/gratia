{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de587efa",
   "metadata": {},
   "source": [
    "# Setup\n",
    "- This notebook uses HF tokenizer and model to feed a sequence 2 sequence for query to item title similariy\n",
    "\n",
    "- Reference: https://pages.github.corp.ebay.com/PyBay/core/dev/explore/bert/tutorials/finetune.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9787224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d082791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thchang/Documents/dev/git/nlp/pytorch-hf\n",
      "\u001b[34msearch_relevance\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34acd087",
   "metadata": {},
   "source": [
    "# Step1: Prepare DataSets\n",
    "- Load into HF dataset from local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b4dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-48b1a6a309eb5012\n",
      "Reusing dataset csv (/Users/thchang/.cache/huggingface/datasets/csv/default-48b1a6a309eb5012/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c20477cb7e45c4b76cef11e82a4119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'title', 'relevance'],\n",
       "        num_rows: 864378\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query', 'title', 'relevance'],\n",
       "        num_rows: 45970\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "DATA_DIR = '/Users/thchang/Documents/dev/git/nlp/pytorch-hf/data/pair_classification/search_relevance/'\n",
    "data = datasets.load_dataset(\n",
    "    'csv',\n",
    "    data_files={\n",
    "        'train': os.path.join(DATA_DIR, 'search_relevance.us.train.csv'),\n",
    "        'dev': os.path.join(DATA_DIR, 'search_relevance.us.dev.csv'),\n",
    "    },\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a487463c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Apple iPhone 7 Plus - 128GB - Gold A1661 BLACKLISTED!!! for parts',\n",
       " 'query': 'blacklisted iphone 7 plus',\n",
       " 'relevance': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4b174",
   "metadata": {},
   "source": [
    "# Step2: Preprocess data \n",
    "- Apply Tokenier\n",
    "- A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b50da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/thchang/Documents/dev/git/nlp/pytorch-hf/huggingface/bert-base-uncased')\n",
    "\n",
    "def preprocess(features):\n",
    "    # Combine query and title separated by SEP\n",
    "    result = tokenizer([' [SEP] '.join([query, title]) for query, title in zip(features['query'], features['title'])])\n",
    "    \n",
    "    # Convert relevance score to an integer label\n",
    "    result['label'] = [int(label) for label in features['relevance'] ]\n",
    "\n",
    "    return result\n",
    "\n",
    "#data = data.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "407144a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "561594c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1],\n",
      " 'input_ids': [101,\n",
      "               2304,\n",
      "               9863,\n",
      "               2098,\n",
      "               18059,\n",
      "               1021,\n",
      "               4606,\n",
      "               102,\n",
      "               6207,\n",
      "               18059,\n",
      "               1021,\n",
      "               4606,\n",
      "               1011,\n",
      "               11899,\n",
      "               18259,\n",
      "               1011,\n",
      "               2751,\n",
      "               17350,\n",
      "               28756,\n",
      "               2487,\n",
      "               2304,\n",
      "               9863,\n",
      "               2098,\n",
      "               999,\n",
      "               999,\n",
      "               999,\n",
      "               2005,\n",
      "               3033,\n",
      "               102],\n",
      " 'label': 1,\n",
      " 'query': 'blacklisted iphone 7 plus',\n",
      " 'relevance': 1.0,\n",
      " 'title': 'Apple iPhone 7 Plus - 128GB - Gold A1661 BLACKLISTED!!! for parts',\n",
      " 'token_type_ids': [0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0]}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e28e7",
   "metadata": {},
   "source": [
    "# Step 3:  Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21a3293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, average_precision_score,\n",
    "                            roc_auc_score, mean_squared_error)\n",
    "\n",
    "# This function will be called to evaluate a prediction, which contains model output and a label\n",
    "def compute_metrics(eval_predictions: EvalPrediction):\n",
    "    # model returns logits of shape [n_samples, n_classes]\n",
    "    logits = eval_predictions.predictions\n",
    "    # logit with the largest value is our class prediction\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    # true label\n",
    "    trues = eval_predictions.label_ids\n",
    "\n",
    "    # We use metrics provided by scikit-learn\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_true=trues, y_pred=predictions),\n",
    "        \"BalancedAccuracy\": balanced_accuracy_score(y_true=trues, y_pred=predictions),\n",
    "        \"AveragePrecision\": average_precision_score(y_true=trues, y_score=logits[:, 1], average='weighted', pos_label=1),\n",
    "        \"ROCAUCScore\": roc_auc_score(y_true=trues, y_score=logits[:, 1], average='weighted'),\n",
    "        \"MSE\": mean_squared_error(y_true=trues, y_pred=predictions, squared=False),\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f421b458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.6666666666666666, 'BalancedAccuracy': 0.5, 'AveragePrecision': 1.0, 'ROCAUCScore': 1.0, 'MSE': 0.5773502691896257}\n"
     ]
    }
   ],
   "source": [
    "prediction = EvalPrediction(\n",
    "    predictions=np.array([[0, 0], [0.0, 0.0], [1.0, 1.0]]),\n",
    "    label_ids=np.array([0, 0, 1])\n",
    ")\n",
    "print(compute_metrics(prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a2ebc",
   "metadata": {},
   "source": [
    "# Step 4:  Prepare Model and Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2762968",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4aef094e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loads a pytorch eBERT model, which is initialized with pre-trained weights.\n",
    "# This model also contains some additional weights, which are used for classification on top of pre-trained BERT output.\n",
    "# Those weights are randomly initialized, so the model needs to be fine-tuned to produce any meaningful output.\n",
    "# Read more https://huggingface.co/transformers/model_doc/auto.html#automodelforsequenceclassification\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "# Download configuration from huggingface.co and cache.\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "model\n",
    "                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17f3b7",
   "metadata": {},
   "source": [
    "### Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6907e523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "# Define an output directory\n",
    "output_dir = os.path.join(os.environ.get('KRYLOV_DATA_DIR', ''), os.environ.get('KRYLOV_WS_PRINCIPAL', ''), 'bert-finetuning')\n",
    "\n",
    "# Learn more about various training arguments https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, 'model_checkpoints'),       # Output model checkpoints\\\n",
    "    num_train_epochs=0.01,                                             # Number of training epochs\\\n",
    "    per_device_train_batch_size=128,                                 # Batch size for training\\\n",
    "    per_device_eval_batch_size=128,                                  # Batch size for evaluation\\\n",
    "    learning_rate=5e-5,                                             # Learning rate\\\n",
    "    warmup_steps=100,                                               # Warmup for learning rate schedule\\\n",
    "    logging_steps=5000,                                             # Logging frequency\\\n",
    "    logging_dir=os.path.join(output_dir, 'model_logs'),             # Directory for logs\\\n",
    "    fp16=False,                                                      # Mixed precision training on V100 GPUs\\\n",
    "    save_strategy=\"epoch\",                                          # Save model checkpoint after each epoch\\\n",
    ")\n",
    "\n",
    "# Learn more about Trainer https://huggingface.co/transformers/main_classes/trainer.html#id1\n",
    "# data_collator will take multiple input entries and create a mini-batch. Because\n",
    "# input sentences can have different length, it will add appropriate padding and\n",
    "# masking, so that the model can efficiently and correctly process it.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['dev'],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385c0ba",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "- It may seem that we lost title, query, relevance, but we really didn't\n",
    "- https://lewtun.github.io/blog/til/nlp/huggingface/transformers/2021/01/15/til-recovering-hidden-trainer-columns.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb4fc3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: title, query, relevance.\n",
      "***** Running training *****\n",
      "  Num examples = 864378\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 68\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 41:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-finetuning/model_checkpoints/checkpoint-68\n",
      "Configuration saved in bert-finetuning/model_checkpoints/checkpoint-68/config.json\n",
      "Model weights saved in bert-finetuning/model_checkpoints/checkpoint-68/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuning/model_checkpoints/checkpoint-68/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuning/model_checkpoints/checkpoint-68/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# This will start a training loop\n",
    "trainer.train()\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7e826",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on a dev set\n",
    "results = trainer.evaluate(data['dev'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a815b5",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and tokenizer to disk\n",
    "trainer.save_model(os.path.join(output_dir, 'my_pretrained_model'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c424cc",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bee3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # make sure to set it back to train if training\n",
    "\n",
    "text = '[CLS] iphone 6 [SEP] iphone 12 [SEP]'\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model(**encoded_input)\n",
    "\n",
    "print('DONE', output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
