// -----------------------------
// Setup
// -----------------------------

<-- Notes
	/Users/chang/Documents/dev/git/opus/04_ml/nlp/huggingface_pytorchligtning/tool-HuggingFace-PyTorchLigtning

<-- Code
	cd /Users/chang/Documents/dev/git/ml/huggingface_pytorchligtening
	source ./venv/bin/activate

	pyenv install 3.9.1
		Mac M1 procesor has some issues; lzma.h not included; 3.9.1 is fine thout
		https://github.com/pyenv/pyenv/issues/1744
		
	pyenv global 3.9.1
		vi .pyenv/version

	which python3 
		should be python3

	python3 -m venv .venv

	source .venv/bin/activate

	pip3 install --upgrade pip

	pip3 install datasets 
	python3 -c "import datasets"

	pip3 install torch

	pip3 install transformers  
	
	pip install pytorch-lightning
  
	pip install jupyter


	Setup a new kernel with my venv
		pip3 install ipykernel 
		.venv/bin/python3 -m ipykernel install --user --name=.venv
			https://towardsdatascience.com/create-virtual-environment-using-virtualenv-and-add-it-to-jupyter-notebook-6e1bf4e03415

	jupyter notebook & 

// -----------------------------
// Datasets (HuggingFace)
// -----------------------------
<-- References
	* https://colab.research.google.com/drive/1YM6wCl2rXC4eX1EUdXZPNZ6_gJ2JgYrg#scrollTo=QvExTIZWvSVw
		Uses Datasets on SQUAD data
		With Bert Transformers
		For both Pytorch and Tensorflow


	* Downloaded dataset is already split into train, test, 

	* dataset includes load_metric, which will automatically include the correct metric to use for the particular task you load
		https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=IreSlFmlIrIm

		Glue benchmark consists of 9 classifications tasks for sentencs and pair of sentinces
		from datasets import load_dataset, load_metric

		We will use the ðŸ¤— Datasets library to download the data and get the metric we need to use for evaluation via load_dataset and load_metric
			from datasets import load_dataset, load_metric
			actual_task = "mnli" if task == "mnli-mm" else task
			dataset: datasets.dataset_dict.DatasetDict = load_dataset("glue", actual_task)
			metric: datasets_modules.metrics.glue = load_metric('glue', actual_task)

			dataset['train']: datasets.arrow_dataset.Dataset 

// -----------------------------
// Transformers (HuggingFace)
// -----------------------------
<-- Tokenizer : 
	Clean text, encode input sentence to be bert compatible (ie id, special tokens)
	
	--> References
		* https://huggingface.co/docs/tokenizers/python/latest/quicktour.html
		* https://huggingface.co/transformers/preprocessing.html
		* https://huggingface.co/transformers/tokenizer_summary.html


	--> Idea: 
			* Tokenizer is actually a pipeline which converts texts to tokens. Then it converts to integer ids to feed to 

			* What does the tokenization pipeline consist of?
				- REFERENCE: https://huggingface.co/docs/tokenizers/python/latest/pipeline.html

				- normalization: 
					Clean the text; strip whitespaces; lower case all text; unicode normalization; remove accented characters

					from tokenizers import normalizers
					from tokenizers.normalizers import NFD, StripAccents
					normalizer = normalizers.Sequence([NFD(), StripAccents()])
	
				- pre-tokenization:
					Split sentence into complete words. Pre-tokenization is the act of splitting a text into smaller objects that give an upper bound to what your tokens will be at the end of training.  Outputs [ (token1, (span1, span2)) ], where span are the offset to the start and end of the token 

					from tokenizers.pre_tokenizers import Whitespace
					pre_tokenizer = Whitespace()
					pre_tokenizer.pre_tokenize_str("Hello! How are you? I'm fine, thank you.")
					# [("Hello", (0, 5)), ("!", (5, 6)), ("How", (7, 10)), ("are", (11, 14)), ("you", (15, 18)),

				- the model:
					Acutally learns how to break your word into tokens. Need training ( WOW !! )  MOdel includes: BPE, Unigram, WordLevel, WordPiece [Ref: https://huggingface.co/docs/tokenizers/python/latest/components.html#models]

					If you want to train you own, need to use the Trainer Class

				- post processing:
					Extension point for further logic, like making the final encoded output suitable for BERT.

					from tokenizers.processors import TemplateProcessing
					tokenizer.post_processor = TemplateProcessing(
					    single="[CLS] $A [SEP]",
					    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
					    special_tokens=[("[CLS]", 1), ("[SEP]", 2)],
					)

			* Let's see the actual code
				Example1:
					# Define tokenizer with model and unknown token
					from tokenizers import Tokenizer
					from tokenizers.models import WordPiece
					bert_tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

					# Define normalization
					from tokenizers import normalizers
					from tokenizers.normalizers import Lowercase, NFD, StripAccents
					bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])

					# Define pre-tokenizer
					from tokenizers.pre_tokenizers import Whitespace
					bert_tokenizer.pre_tokenizer = Whitespace()

					# Post processing for Ebert compatibility
					from tokenizers.processors import TemplateProcessing
					bert_tokenizer.post_processor = TemplateProcessing(
					    single="[CLS] $A [SEP]",
					    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
					    special_tokens=[
					        ("[CLS]", 1),
					        ("[SEP]", 2),
					    ],
					)

				Example2:
					from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers
					tokenizer = Tokenizer(models.Unigram())
					tokenizer.normalizer = normalizers.NFKC()
					tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
					tokenizer.decoders = decoders.ByteLevel()

					trainer = trainers.UnigramTrainer(
					    vocab_size=20000,
					    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
					    special_tokens=["<PAD>", "<BOS>", "<EOS>"],
					)

				Example3: Use a pre-existing tokenizer
					from tokenizers import BertWordPieceTokenizer, but you need to have the vocabulary file
					tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)



<-- Bert Transformers
	--> References:
		https://huggingface.co/transformers/

	--> Idea | Overview | Goals
		* Transformers provides 
			- general purpose architectures (BERT, GPT-2, RoBerta, XLM, DistilBert, XLNet) f
			- for Natural Language Understanding (NLU) and Natural Language Generation (NLG)
			- for multiple languages (100+)
			- for inter-operability between Pytorch, Tensorflow, and JAX implementations

		* Be as easy and fast to use as possible
			- Only 3 abstraction classes: configuration, model, and tokenizer
				* Model class are Pytorch models or Keras Modelthat works with pre-trained weights provided in the library
				* Config store ALL parameters requried to build a model
				* Tokenizer store the vocabulary and enables encoding/decoding of STRINGS to list of embedding indices to be fed to the class
			
			- These 3 classes can be instantiated form the common from_pretrained() api, which downloads, cache from a pretrained checkpoints on Hugging Face locally
			
			- 2 higher level abstractions
				* pipeline : abstraction for a combination of tokenizer + model + config

				* Trainer/TFRainer: abstraction to quickly train or fine tune a model

			- Hugginfface is not a tool to build nerual nets; if you want to build or extend upon the huggingface model, use Pytorch|Tensorflow and inhert from base class in HF library to reuse model loading/savings.

		* Expose model's internals as consistently as possible
			- Access with single API to full hidden-states and attention weights
			- Tokenizer and base model API are standardized so one can experiment between models

		* Switch between Pytorch or TF inferencing, which is ideal for some systems that supports only one

	--> fine Tuning
		https://huggingface.co/transformers/training.html


// -----------------------------
// Pytorch Ligtening
// -----------------------------
<-- Refernces
	* https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/notebooks/04-transformers-text-classification.ipynb

	* https://github.com/PyTorchLightning/pytorch-lightning